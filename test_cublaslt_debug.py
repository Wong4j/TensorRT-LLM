#!/usr/bin/env python3
"""
è°ƒè¯• cuBLASLt FP4 GEMM çš„ç®€å•æµ‹è¯•è„šæœ¬
"""

import logging
import torch
import traceback

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_cublaslt_debug():
    """è°ƒè¯• cuBLASLt åç«¯"""
    logger.info("=" * 60)
    logger.info("cuBLASLt FP4 GEMM è°ƒè¯•æµ‹è¯•")
    logger.info("=" * 60)
    
    try:
        from tensorrt_llm._torch.custom_ops import nvfp4_gemm
        import tensorrt_llm.quantization.utils.fp4_utils as fp4_utils
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        m, n, k = 32, 64, 128
        k_compressed = k // 2
        scale_groups = k_compressed // 16
        
        logger.info(f"æµ‹è¯•çŸ©é˜µå¤§å°: m={m}, n={n}, k={k}")
        logger.info(f"å‹ç¼©å K: {k_compressed}, ç¼©æ”¾å› å­ç»„æ•°: {scale_groups}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        act_fp4 = torch.randint(0, 255, (m, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        weight = torch.randint(0, 255, (n, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        act_sf = torch.ones((m, scale_groups), dtype=torch.float8_e4m3fn, device="cuda")
        weight_scale = torch.ones((n, scale_groups), dtype=torch.float8_e4m3fn, device="cuda")
        alpha = torch.tensor(1.0, dtype=torch.float32, device="cuda")
        
        logger.info("è¾“å…¥æ•°æ®åˆ›å»ºå®Œæˆ")
        logger.info(f"  act_fp4: {act_fp4.shape}, {act_fp4.dtype}")
        logger.info(f"  weight: {weight.shape}, {weight.dtype}")
        logger.info(f"  act_sf: {act_sf.shape}, {act_sf.dtype}")
        logger.info(f"  weight_scale: {weight_scale.shape}, {weight_scale.dtype}")
        
        # æµ‹è¯• cuBLASLt åç«¯
        logger.info("å¼€å§‹æµ‹è¯• cuBLASLt åç«¯...")
        
        try:
            result = nvfp4_gemm(
                act_fp4=act_fp4,
                weight=weight,
                act_sf=act_sf,
                weight_scale=weight_scale,
                alpha=alpha,
                output_dtype=torch.bfloat16,
                to_userbuffers=False,
                backend="cublaslt"
            )
            
            logger.info("âœ… cuBLASLt åç«¯æµ‹è¯•æˆåŠŸï¼")
            logger.info(f"è¾“å‡ºå½¢çŠ¶: {result.shape}")
            logger.info(f"è¾“å‡ºç±»å‹: {result.dtype}")
            logger.info(f"è¾“å‡ºè®¾å¤‡: {result.device}")
            
        except Exception as e:
            logger.error(f"âŒ cuBLASLt åç«¯æµ‹è¯•å¤±è´¥: {e}")
            logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            logger.error(traceback.format_exc())
            
            # å°è¯•è·å–æ›´è¯¦ç»†çš„ CUDA é”™è¯¯ä¿¡æ¯
            if hasattr(e, 'cudaError'):
                logger.error(f"CUDA é”™è¯¯ä»£ç : {e.cudaError}")
            
            return False
        
        return True
        
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        logger.error(traceback.format_exc())
        return False
    except Exception as e:
        logger.error(f"âŒ æ„å¤–é”™è¯¯: {e}")
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = test_cublaslt_debug()
    if success:
        print("\nğŸ‰ æµ‹è¯•æˆåŠŸï¼")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼")
    exit(0 if success else 1)
