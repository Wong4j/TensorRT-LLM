#!/usr/bin/env python3
"""
æµ‹è¯• cuBLASLt FP4 GEMM æ˜¯å¦å¯ç”¨
"""

import torch
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_cublaslt_fp4_gemm_enabled():
    """æµ‹è¯• cuBLASLt FP4 GEMM æ˜¯å¦å¯ç”¨"""
    logger.info("Testing cuBLASLt FP4 GEMM availability...")
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ cublaslt_nvfp4_gemm æ“ä½œ
        if hasattr(torch.ops.trtllm, 'cublaslt_nvfp4_gemm'):
            logger.info("âœ… cuBLASLt FP4 GEMM operation is available")
            return True
        else:
            logger.error("âŒ cuBLASLt FP4 GEMM operation is NOT available")
            return False
    except Exception as e:
        logger.error(f"âŒ Error checking cuBLASLt FP4 GEMM: {e}")
        return False

def test_compile_time_flags():
    """æµ‹è¯•ç¼–è¯‘æ—¶æ ‡å¿—"""
    logger.info("Checking compile-time flags...")
    
    try:
        # å°è¯•å¯¼å…¥ç›¸å…³æ¨¡å—
        import tensorrt_llm.quantization.utils.fp4_utils as fp4_utils
        logger.info("âœ… FP4 utils module available")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹å®šä¹‰
        logger.info(f"FLOAT4_E2M1X2 dtype: {fp4_utils.FLOAT4_E2M1X2}")
        
        return True
    except Exception as e:
        logger.error(f"âŒ Error importing FP4 utils: {e}")
        return False

def test_cutlass_backend():
    """æµ‹è¯• CUTLASS åç«¯æ˜¯å¦å¯ç”¨"""
    logger.info("Testing CUTLASS backend...")
    
    try:
        from tensorrt_llm._torch.custom_ops import nvfp4_gemm
        import tensorrt_llm.quantization.utils.fp4_utils as fp4_utils
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        m, n, k = 64, 128, 256
        k_compressed = k // 2  # FP4 å‹ç¼©äº† K ç»´åº¦
        
        act_fp4 = torch.randint(0, 255, (m, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        weight = torch.randint(0, 255, (n, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        # nvfp4: æ¯ 16 ä¸ªå…ƒç´ å…±äº«ä¸€ä¸ªç¼©æ”¾å› å­
        # CUTLASS æœŸæœ› uint8 ç±»å‹ (cutlass::float_ue4m3_t)
        act_sf = torch.ones((m, k_compressed // 16), dtype=torch.uint8, device="cuda")
        weight_scale = torch.ones((n, k_compressed // 16), dtype=torch.uint8, device="cuda")
        alpha = torch.tensor(1.0, dtype=torch.float32, device="cuda")
        
        logger.info("Testing CUTLASS backend...")
        try:
            result = nvfp4_gemm(
                act_fp4=act_fp4,
                weight=weight,
                act_sf=act_sf,
                weight_scale=weight_scale,
                alpha=alpha,
                output_dtype=torch.bfloat16,
                to_userbuffers=False,
                backend="cutlass"
            )
            logger.info("âœ… CUTLASS backend test successful")
            logger.info(f"Output shape: {list(result.shape)}, dtype: {result.dtype}")
            return True
        except Exception as e:
            logger.error(f"âŒ CUTLASS backend test failed: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Error testing CUTLASS backend: {e}")
        return False

def test_cublaslt_backend():
    """æµ‹è¯• cuBLASLt åç«¯æ˜¯å¦å¯ç”¨"""
    logger.info("Testing cuBLASLt backend...")
    
    try:
        from tensorrt_llm._torch.custom_ops import nvfp4_gemm
        import tensorrt_llm.quantization.utils.fp4_utils as fp4_utils
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        m, n, k = 64, 128, 256
        k_compressed = k // 2  # FP4 å‹ç¼©äº† K ç»´åº¦
        
        act_fp4 = torch.randint(0, 255, (m, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        weight = torch.randint(0, 255, (n, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        # nvfp4: æ¯ 16 ä¸ªå…ƒç´ å…±äº«ä¸€ä¸ªç¼©æ”¾å› å­
        # cuBLASLt æœŸæœ› fp8_e4m3fn ç±»å‹
        act_sf = torch.ones((m, k_compressed // 16), dtype=torch.float8_e4m3fn, device="cuda")
        weight_scale = torch.ones((n, k_compressed // 16), dtype=torch.float8_e4m3fn, device="cuda")
        alpha = torch.tensor(1.0, dtype=torch.float32, device="cuda")
        
        logger.info("Testing cuBLASLt backend...")
        try:
            result = nvfp4_gemm(
                act_fp4=act_fp4,
                weight=weight,
                act_sf=act_sf,
                weight_scale=weight_scale,
                alpha=alpha,
                output_dtype=torch.bfloat16,
                to_userbuffers=False,
                backend="cublaslt"
            )
            logger.info("âœ… cuBLASLt backend test successful")
            logger.info(f"Output shape: {list(result.shape)}, dtype: {result.dtype}")
            return True
        except Exception as e:
            logger.error(f"âŒ cuBLASLt backend test failed: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Error testing cuBLASLt backend: {e}")
        return False

def test_backend_comparison():
    """å¯¹æ¯”æµ‹è¯• CUTLASS å’Œ cuBLASLt ä¸¤ä¸ªåç«¯"""
    logger.info("Comparing CUTLASS and cuBLASLt backends...")
    
    try:
        from tensorrt_llm._torch.custom_ops import nvfp4_gemm
        import tensorrt_llm.quantization.utils.fp4_utils as fp4_utils
        import time
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        m, n, k = 128, 256, 512
        k_compressed = k // 2  # FP4 å‹ç¼©äº† K ç»´åº¦
        
        act_fp4 = torch.randint(0, 255, (m, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        weight = torch.randint(0, 255, (n, k_compressed), dtype=fp4_utils.FLOAT4_E2M1X2, device="cuda")
        # ä¸ºä¸åŒåç«¯åˆ›å»ºä¸åŒæ•°æ®ç±»å‹çš„ç¼©æ”¾å› å­
        act_sf_uint8 = torch.ones((m, k_compressed // 16), dtype=torch.uint8, device="cuda")  # CUTLASS
        weight_scale_uint8 = torch.ones((n, k_compressed // 16), dtype=torch.uint8, device="cuda")  # CUTLASS
        act_sf_fp8 = torch.ones((m, k_compressed // 16), dtype=torch.float8_e4m3fn, device="cuda")  # cuBLASLt
        weight_scale_fp8 = torch.ones((n, k_compressed // 16), dtype=torch.float8_e4m3fn, device="cuda")  # cuBLASLt
        alpha = torch.tensor(1.0, dtype=torch.float32, device="cuda")
        
        results = {}
        
        # æµ‹è¯• CUTLASS åç«¯
        logger.info("Testing CUTLASS backend...")
        try:
            start_time = time.time()
            result_cutlass = nvfp4_gemm(
                act_fp4=act_fp4,
                weight=weight,
                act_sf=act_sf_uint8,  # CUTLASS ä½¿ç”¨ uint8
                weight_scale=weight_scale_uint8,  # CUTLASS ä½¿ç”¨ uint8
                alpha=alpha,
                output_dtype=torch.bfloat16,
                to_userbuffers=False,
                backend="cutlass"
            )
            cutlass_time = time.time() - start_time
            results['cutlass'] = {
                'success': True,
                'shape': list(result_cutlass.shape),
                'dtype': result_cutlass.dtype,
                'time': cutlass_time
            }
            logger.info(f"âœ… CUTLASS: shape={list(result_cutlass.shape)}, dtype={result_cutlass.dtype}, time={cutlass_time:.4f}s")
        except Exception as e:
            results['cutlass'] = {'success': False, 'error': str(e)}
            logger.error(f"âŒ CUTLASS failed: {e}")
        
        # æµ‹è¯• cuBLASLt åç«¯
        logger.info("Testing cuBLASLt backend...")
        try:
            start_time = time.time()
            result_cublaslt = nvfp4_gemm(
                act_fp4=act_fp4,
                weight=weight,
                act_sf=act_sf_fp8,  # cuBLASLt ä½¿ç”¨ fp8_e4m3fn
                weight_scale=weight_scale_fp8,  # cuBLASLt ä½¿ç”¨ fp8_e4m3fn
                alpha=alpha,
                output_dtype=torch.bfloat16,
                to_userbuffers=False,
                backend="cublaslt"
            )
            cublaslt_time = time.time() - start_time
            results['cublaslt'] = {
                'success': True,
                'shape': list(result_cublaslt.shape),
                'dtype': result_cublaslt.dtype,
                'time': cublaslt_time
            }
            logger.info(f"âœ… cuBLASLt: shape={list(result_cublaslt.shape)}, dtype={result_cublaslt.dtype}, time={cublaslt_time:.4f}s")
        except Exception as e:
            results['cublaslt'] = {'success': False, 'error': str(e)}
            logger.error(f"âŒ cuBLASLt failed: {e}")
        
        # å¯¹æ¯”ç»“æœ
        logger.info("=" * 50)
        logger.info("Backend Comparison Results:")
        logger.info("=" * 50)
        
        for backend, result in results.items():
            if result['success']:
                logger.info(f"{backend.upper()}: âœ… Success")
                logger.info(f"  Shape: {result['shape']}")
                logger.info(f"  Dtype: {result['dtype']}")
                logger.info(f"  Time:  {result['time']:.4f}s")
            else:
                logger.info(f"{backend.upper()}: âŒ Failed - {result['error']}")
        
        # å¦‚æœä¸¤ä¸ªåç«¯éƒ½æˆåŠŸï¼Œæ¯”è¾ƒæ€§èƒ½
        if results['cutlass']['success'] and results['cublaslt']['success']:
            cutlass_time = results['cutlass']['time']
            cublaslt_time = results['cublaslt']['time']
            speedup = cutlass_time / cublaslt_time if cublaslt_time > 0 else float('inf')
            logger.info(f"Performance comparison:")
            logger.info(f"  CUTLASS time:    {cutlass_time:.4f}s")
            logger.info(f"  cuBLASLt time:   {cublaslt_time:.4f}s")
            logger.info(f"  Speedup ratio:   {speedup:.2f}x")
        
        return results['cutlass']['success'] and results['cublaslt']['success']
        
    except Exception as e:
        logger.error(f"âŒ Error in backend comparison: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("FP4 GEMM åç«¯å¯¹æ¯”æµ‹è¯•")
    logger.info("=" * 60)
    
    # æµ‹è¯•ç¼–è¯‘æ—¶æ ‡å¿—
    compile_ok = test_compile_time_flags()
    
    # æµ‹è¯•æ“ä½œå¯ç”¨æ€§
    ops_ok = test_cublaslt_fp4_gemm_enabled()
    
    # æµ‹è¯• CUTLASS åç«¯
    cutlass_ok = test_cutlass_backend()
    
    # æµ‹è¯• cuBLASLt åç«¯
    cublaslt_ok = test_cublaslt_backend()
    
    # å¯¹æ¯”æµ‹è¯•ä¸¤ä¸ªåç«¯
    comparison_ok = test_backend_comparison()
    
    logger.info("=" * 60)
    logger.info("æµ‹è¯•ç»“æœæ€»ç»“:")
    logger.info(f"ç¼–è¯‘æ—¶æ ‡å¿—: {'âœ… é€šè¿‡' if compile_ok else 'âŒ å¤±è´¥'}")
    logger.info(f"æ“ä½œå¯ç”¨æ€§: {'âœ… é€šè¿‡' if ops_ok else 'âŒ å¤±è´¥'}")
    logger.info(f"CUTLASS åç«¯: {'âœ… é€šè¿‡' if cutlass_ok else 'âŒ å¤±è´¥'}")
    logger.info(f"cuBLASLt åç«¯: {'âœ… é€šè¿‡' if cublaslt_ok else 'âŒ å¤±è´¥'}")
    logger.info(f"åç«¯å¯¹æ¯”: {'âœ… é€šè¿‡' if comparison_ok else 'âŒ å¤±è´¥'}")
    
    if compile_ok and ops_ok and cutlass_ok and cublaslt_ok and comparison_ok:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CUTLASS å’Œ cuBLASLt ä¸¤ä¸ªåç«¯éƒ½å¯ç”¨ï¼")
        return True
    else:
        logger.error("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
