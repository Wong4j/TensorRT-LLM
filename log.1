2025-09-13 14:13:23,062 - __main__ - INFO - ================================================================================
2025-09-13 14:13:23,062 - __main__ - INFO - FP4 GEMM 后端对比测试
2025-09-13 14:13:23,062 - __main__ - INFO - ================================================================================
2025-09-13 14:13:25,989 - __main__ - INFO - ============================================================
2025-09-13 14:13:25,989 - __main__ - INFO - Testing different matrix sizes
2025-09-13 14:13:25,989 - __main__ - INFO - ============================================================
2025-09-13 14:13:25,989 - __main__ - INFO - 
Testing size: m=1024, n=1024, k=1024
2025-09-13 14:13:29,889 - numexpr.utils - INFO - Note: detected 224 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2025-09-13 14:13:29,889 - numexpr.utils - INFO - Note: NumExpr detected 224 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-09-13 14:13:29,889 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-09-13 14:13:30,634 - datasets - INFO - PyTorch version 2.8.0a0+5228986c39.nv25.6 available.
2025-09-13 14:13:30,634 - datasets - INFO - Polars version 1.25.2 available.
/usr/local/lib/python3.12/dist-packages/modelopt/torch/__init__.py:36: UserWarning: transformers version 4.55.0 is incompatible with nvidia-modelopt and may cause issues. Please install recommended version with `pip install nvidia-modelopt[hf]` if working with HF models.
  _warnings.warn(
2025-09-13 14:13:32,741 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[TensorRT-LLM] TensorRT-LLM version: 1.1.0rc4
2025-09-13 14:13:33,427 - __main__ - INFO - Benchmarking CUTLASS backend...
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,427 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,443 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[1024, 1024], dtype=torch.bfloat16
2025-09-13 14:13:33,443 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[1024, 1024], dtype=torch.bfloat16
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[1024, 1024], dtype=torch.bfloat16
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[1024, 1024], dtype=torch.bfloat16
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,444 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[1024, 1024], dtype=torch.bfloat16
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[1024, 512], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[1024, 32], dtype=torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[1024, 1024], dtype=torch.bfloat16
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,445 - __main__ - INFO - Benchmarking CUBLASLT backend...
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [1024, 1024]
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [1024, 1024], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,445 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,472 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,472 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,472 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,472 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [1024, 1024]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [1024, 1024], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [1024, 1024]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [1024, 1024], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [1024, 1024]
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [1024, 1024], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,473 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [1024, 1024]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [1024, 1024], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [1024, 512], weight: [1024, 512]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [1024, 32], weight_scale: [1024, 32]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [1024, 1024]
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [1024, 1024], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [1024, 512]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat2: [1024, 512]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [1024]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [1024, 1024]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=1024, n=1024, k=1024, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=1024, n=1024, k=1024
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 268435456 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,474 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [1024, 1024], dtype: torch.bfloat16
2025-09-13 14:13:33,474 - __main__ - INFO - CUTLASS:    0.0003s ± 0.0001s
2025-09-13 14:13:33,474 - __main__ - INFO - cuBLASLt:   0.0004s ± 0.0001s
2025-09-13 14:13:33,474 - __main__ - INFO - Speedup:    0.77x
2025-09-13 14:13:33,513 - __main__ - INFO - Max abs diff: 0.000000
2025-09-13 14:13:33,514 - __main__ - INFO - Max rel diff: 0.000000
2025-09-13 14:13:33,514 - __main__ - INFO - Within tolerance: True
2025-09-13 14:13:33,522 - __main__ - INFO - CUTLASS output range: [0.000000, 0.000000]
2025-09-13 14:13:33,523 - __main__ - INFO - cuBLASLt output range: [0.000000, 0.000000]
2025-09-13 14:13:33,523 - __main__ - INFO - CUTLASS output mean: 0.000000
2025-09-13 14:13:33,523 - __main__ - INFO - cuBLASLt output mean: 0.000000
2025-09-13 14:13:33,523 - __main__ - INFO - CUTLASS has NaN: False, has Inf: False
2025-09-13 14:13:33,523 - __main__ - INFO - cuBLASLt has NaN: False, has Inf: False
2025-09-13 14:13:33,523 - __main__ - INFO - 
Testing size: m=2048, n=2048, k=2048
2025-09-13 14:13:33,524 - __main__ - INFO - Benchmarking CUTLASS backend...
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[2048, 2048], dtype=torch.bfloat16
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[2048, 2048], dtype=torch.bfloat16
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,524 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[2048, 2048], dtype=torch.bfloat16
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[2048, 2048], dtype=torch.bfloat16
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[2048, 2048], dtype=torch.bfloat16
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[2048, 1024], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[2048, 64], dtype=torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[2048, 2048], dtype=torch.bfloat16
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,525 - __main__ - INFO - Benchmarking CUBLASLT backend...
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,525 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [2048, 2048]
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [2048, 2048], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,526 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [2048, 2048]
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [2048, 2048], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,533 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [2048, 2048]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [2048, 2048], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [2048, 2048]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [2048, 2048], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [2048, 2048]
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [2048, 2048], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,534 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [2048, 1024], weight: [2048, 1024]
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [2048, 64], weight_scale: [2048, 64]
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [2048, 2048]
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [2048, 2048], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [2048, 1024]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat2: [2048, 1024]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [2048]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [2048, 2048]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=2048, n=2048, k=2048, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=2048, n=2048, k=2048
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,535 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [2048, 2048], dtype: torch.bfloat16
2025-09-13 14:13:33,535 - __main__ - INFO - CUTLASS:    0.0003s ± 0.0000s
2025-09-13 14:13:33,535 - __main__ - INFO - cuBLASLt:   0.0003s ± 0.0000s
2025-09-13 14:13:33,535 - __main__ - INFO - Speedup:    0.84x
2025-09-13 14:13:33,535 - __main__ - INFO - Max abs diff: 0.000000
2025-09-13 14:13:33,535 - __main__ - INFO - Max rel diff: 0.000000
2025-09-13 14:13:33,535 - __main__ - INFO - Within tolerance: True
2025-09-13 14:13:33,535 - __main__ - INFO - CUTLASS output range: [0.000000, 0.000000]
2025-09-13 14:13:33,535 - __main__ - INFO - cuBLASLt output range: [0.000000, 0.000000]
2025-09-13 14:13:33,535 - __main__ - INFO - CUTLASS output mean: 0.000000
2025-09-13 14:13:33,535 - __main__ - INFO - cuBLASLt output mean: 0.000000
2025-09-13 14:13:33,536 - __main__ - INFO - CUTLASS has NaN: False, has Inf: False
2025-09-13 14:13:33,536 - __main__ - INFO - cuBLASLt has NaN: False, has Inf: False
2025-09-13 14:13:33,536 - __main__ - INFO - 
Testing size: m=4096, n=4096, k=4096
2025-09-13 14:13:33,536 - __main__ - INFO - Benchmarking CUTLASS backend...
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[4096, 4096], dtype=torch.bfloat16
2025-09-13 14:13:33,536 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[4096, 4096], dtype=torch.bfloat16
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[4096, 4096], dtype=torch.bfloat16
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[4096, 4096], dtype=torch.bfloat16
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,537 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[4096, 4096], dtype=torch.bfloat16
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[4096, 2048], dtype=torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[4096, 128], dtype=torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[4096, 4096], dtype=torch.bfloat16
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,538 - __main__ - INFO - Benchmarking CUBLASLT backend...
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [4096, 4096]
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [4096, 4096], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,538 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [4096, 4096]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [4096, 4096], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [4096, 4096]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [4096, 4096], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [4096, 4096]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [4096, 4096], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,539 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [4096, 4096]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [4096, 4096], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [4096, 2048], weight: [4096, 2048]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [4096, 128], weight_scale: [4096, 128]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [4096, 4096]
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [4096, 4096], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [4096, 2048]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat2: [4096, 2048]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [4096]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [4096, 4096]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=4096, n=4096, k=4096, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=4096, n=4096, k=4096
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 1048576 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,540 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [4096, 4096], dtype: torch.bfloat16
2025-09-13 14:13:33,540 - __main__ - INFO - CUTLASS:    0.0003s ± 0.0000s
2025-09-13 14:13:33,540 - __main__ - INFO - cuBLASLt:   0.0003s ± 0.0000s
2025-09-13 14:13:33,540 - __main__ - INFO - Speedup:    0.84x
2025-09-13 14:13:33,541 - __main__ - INFO - Max abs diff: 0.000000
2025-09-13 14:13:33,541 - __main__ - INFO - Max rel diff: 0.000000
2025-09-13 14:13:33,541 - __main__ - INFO - Within tolerance: True
2025-09-13 14:13:33,541 - __main__ - INFO - CUTLASS output range: [0.000000, 0.000000]
2025-09-13 14:13:33,541 - __main__ - INFO - cuBLASLt output range: [0.000000, 0.000000]
2025-09-13 14:13:33,541 - __main__ - INFO - CUTLASS output mean: 0.000000
2025-09-13 14:13:33,541 - __main__ - INFO - cuBLASLt output mean: 0.000000
2025-09-13 14:13:33,541 - __main__ - INFO - CUTLASS has NaN: False, has Inf: False
2025-09-13 14:13:33,541 - __main__ - INFO - cuBLASLt has NaN: False, has Inf: False
2025-09-13 14:13:33,541 - __main__ - INFO - ============================================================
2025-09-13 14:13:33,541 - __main__ - INFO - Testing different output data types
2025-09-13 14:13:33,541 - __main__ - INFO - ============================================================
2025-09-13 14:13:33,541 - __main__ - INFO - 
Testing output dtype: torch.float16
2025-09-13 14:13:33,541 - __main__ - INFO - Benchmarking CUTLASS backend...
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,542 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,548 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float16
2025-09-13 14:13:33,548 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,548 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,548 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,548 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,548 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float16
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float16
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float16
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,549 - __main__ - INFO - Benchmarking CUBLASLT backend...
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,549 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float16, device: cuda:0
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 5
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using half precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 6__half
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float16, device: cuda:0
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 5
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using half precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 6__half
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float16, device: cuda:0
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 5
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using half precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 6__half
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float16, to_userbuffers: False
2025-09-13 14:13:33,550 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float16, device: cuda:0
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 5
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using half precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 6__half
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 6__half
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float16
2025-09-13 14:13:33,551 - __main__ - INFO - CUTLASS:    0.0003s
2025-09-13 14:13:33,551 - __main__ - INFO - cuBLASLt:   0.0003s
2025-09-13 14:13:33,551 - __main__ - INFO - Output shape: [128, 256]
2025-09-13 14:13:33,551 - __main__ - INFO - Output dtype: torch.float16
2025-09-13 14:13:33,551 - __main__ - INFO - 
Testing output dtype: torch.bfloat16
2025-09-13 14:13:33,551 - __main__ - INFO - Benchmarking CUTLASS backend...
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.bfloat16
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.bfloat16
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,551 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.bfloat16
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.bfloat16
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.bfloat16
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,552 - __main__ - INFO - Benchmarking CUBLASLT backend...
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,552 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.bfloat16, to_userbuffers: False
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.bfloat16, device: cuda:0
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 15
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using bfloat16 precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: 13__nv_bfloat16
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.bfloat16
2025-09-13 14:13:33,553 - __main__ - INFO - CUTLASS:    0.0003s
2025-09-13 14:13:33,553 - __main__ - INFO - cuBLASLt:   0.0003s
2025-09-13 14:13:33,553 - __main__ - INFO - Output shape: [128, 256]
2025-09-13 14:13:33,553 - __main__ - INFO - Output dtype: torch.bfloat16
2025-09-13 14:13:33,553 - __main__ - INFO - 
Testing output dtype: torch.float32
2025-09-13 14:13:33,553 - __main__ - INFO - Benchmarking CUTLASS backend...
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,553 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float32
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float32
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float32
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,560 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float32
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cutlass
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM inputs:
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1 (act_fp4): shape=[128, 256], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2 (weight): shape=[256, 256], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat1_scale: shape=[128, 16], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   mat2_scale: shape=[256, 16], dtype=torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   global_scale: shape=[], dtype=torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   tactic: -1, to_userbuffers: False
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward]   fp4_gemm_type: 0, output_dtype: torch.float32
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: CUTLASS
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [runGemm] Using CUTLASS backend for FP4 GEMM
[TensorRT-LLM][INFO] [runGemm] FP4 GEMM type: 0
[TensorRT-LLM][INFO] [runGemm] GEMM dimensions: m=128, n=256, k=512, batch_count=1
[TensorRT-LLM][INFO] [runGemm] Using W4A4_NVFP4_NVFP4 CUTLASS runner
[TensorRT-LLM][INFO] [runGemm] CUTLASS workspace size: 0 bytes
[TensorRT-LLM][INFO] [runGemm] Executing CUTLASS W4A4_NVFP4_NVFP4 GEMM
[TensorRT-LLM][INFO] [runGemm] CUTLASS W4A4_NVFP4_NVFP4 GEMM completed
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [FP4GemmRunner::forward] CUTLASS FP4 GEMM output: shape=[128, 256], dtype=torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] CUTLASS output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,561 - __main__ - INFO - Benchmarking CUBLASLT backend...
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float32, device: cuda:0
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using float precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: f
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,561 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float32, device: cuda:0
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using float precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: f
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float32, device: cuda:0
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using float precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: f
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Backend: cublaslt
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input shapes - act_fp4: [128, 256], weight: [256, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Input dtypes - act_fp4: torch.uint8, weight: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale shapes - act_sf: [128, 16], weight_scale: [256, 16]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Scale dtypes - act_sf: torch.uint8, weight_scale: torch.uint8
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Alpha shape: [], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Output dtype: torch.float32, to_userbuffers: False
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Creating output tensor with shape: [128, 256]
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Created output tensor - shape: [128, 256], dtype: torch.float32, device: cuda:0
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] Calling C++ cuBLASLt implementation with swapped inputs...
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor shapes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 (act_fp4): [256, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 (weight): [128, 256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale: [0]
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Input tensor dtypes:
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2 dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat1Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   mat2Scale dtype: 0
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   globalScale dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm]   output dtype: 6
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] Using float precision template
[TensorRT-LLM][INFO] [runGemm] Starting FP4 GEMM execution
[TensorRT-LLM][INFO] [runGemm] Backend: cuBLASLt
[TensorRT-LLM][INFO] [runGemm] Output type: f
[TensorRT-LLM][INFO] [runGemm] Input tensor shapes:
[TensorRT-LLM][INFO] [runGemm]   mat1: [256, 256]
[TensorRT-LLM][INFO] [runGemm]   mat2: [128, 256]
[TensorRT-LLM][INFO] [runGemm]   mat1Scale: [256]
[TensorRT-LLM][INFO] [runGemm]   mat2Scale: [128]
[TensorRT-LLM][INFO] [runGemm]   globalScale: [0]
[TensorRT-LLM][INFO] [runGemm]   output: [128, 256]
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Starting cuBLASLt FP4 GEMM
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] GEMM dimensions: m=256, n=128, k=512, batch_count=1
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Workspace size: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Output type: f
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] Input scaling factor type: uint8
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Starting cuBLASLt execution
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Matrix dimensions: m=256, n=128, k=512
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Workspace: 4194304 bytes
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating operation descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating matrix descriptors
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Creating preference descriptor
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Getting heuristic algorithm
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Found 1 suitable algorithms
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] Executing cuBLASLt matmul
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::executeCublasLtGemm] cuBLASLt matmul completed successfully
[TensorRT-LLM][INFO] [CublasLtFp4GemmRunner::gemm] cuBLASLt FP4 GEMM completed successfully
[TensorRT-LLM][INFO] [cublaslt_nvfp4_gemm] GEMM computation completed successfully
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [cublaslt_nvfp4_gemm_impl] C++ cuBLASLt call completed. Output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,562 - tensorrt_llm._torch.custom_ops.torch_custom_ops - INFO - [nvfp4_gemm] cuBLASLt output shape: [128, 256], dtype: torch.float32
2025-09-13 14:13:33,563 - __main__ - INFO - CUTLASS:    0.0003s
2025-09-13 14:13:33,563 - __main__ - INFO - cuBLASLt:   0.0004s
2025-09-13 14:13:33,563 - __main__ - INFO - Output shape: [128, 256]
2025-09-13 14:13:33,563 - __main__ - INFO - Output dtype: torch.float32
2025-09-13 14:13:33,563 - __main__ - INFO - ================================================================================
2025-09-13 14:13:33,563 - __main__ - INFO - 测试完成！
2025-09-13 14:13:33,563 - __main__ - INFO - ================================================================================
